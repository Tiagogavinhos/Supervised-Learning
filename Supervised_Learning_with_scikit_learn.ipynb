{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOStLwky4kGC7+Mzdlh6jxK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiagogavinhos/Supervised-Learning/blob/main/Supervised_Learning_with_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **k-Nearest Neighbors: Fit**\n",
        "\n",
        "\n",
        "In this exercise, you will build your first classification model using the churn_df dataset, which has been preloaded for the remainder of the chapter.\n",
        "\n",
        "The target, \"churn\", needs to be a single column with the same number of observations as the feature data. The feature data has already been converted into numpy arrays.\n",
        "\n",
        "\"account_length\" and \"customer_service_calls\" are treated as features because account length indicates customer loyalty, and frequent customer service calls may signal dissatisfaction, both of which can be good predictors of churn.\n",
        "\n",
        "#**k-Nearest Neighbors: Predict**\n",
        "\n",
        "\n",
        "Now you have fit a KNN classifier, you can use it to predict the label of new data points. All available data was used for training, however, fortunately, there are new observations available. These have been preloaded for you as X_new.\n",
        "\n",
        "The model knn, which you created and fit the data in the last exercise, has been preloaded for you. You will use your classifier to predict the labels of a set of new data points:\n",
        "\n",
        "X_new = np.array([[30.0, 17.5],\n",
        "                  [107.0, 24.1],\n",
        "                  [213.0, 10.9]])\n",
        "\n",
        "\n",
        "#**Train/test split + computing accuracy**\n",
        "\n",
        "It's time to practice splitting your data into training and test sets with the churn_df dataset!\n",
        "\n",
        "NumPy arrays have been created for you containing the features as X and the target variable as y.\n",
        "\n",
        "#**Overfitting and underfitting**\n",
        "\n",
        "Interpreting model complexity is a great way to evaluate supervised learning performance. Your aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations.\n",
        "\n",
        "The training and test sets have been created from the churn_df dataset and preloaded as X_train, X_test, y_train, and y_test.\n",
        "\n",
        "In addition, KNeighborsClassifier has been imported for you along with numpy as np.\n",
        "\n",
        "Visualizing model complexity\n",
        "Now you have calculated the accuracy of the KNN model on the training and test sets using various values of n_neighbors, you can create a model complexity curve to visualize how performance changes as the model becomes less complex!\n",
        "\n",
        "The variables neighbors, train_accuracies, and test_accuracies, which you generated in the previous exercise, have all been preloaded for you. You will plot the results to aid in finding the optimal number of neighbors for your model."
      ],
      "metadata": {
        "id": "H-1_8xR4bZtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "########## Datacamp\n",
        "######################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## The classification challenge\n",
        "######################################################\n",
        "########## k-Nearest Neighbors: Fit\n",
        "######################################################\n",
        "\n",
        "Import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "y = churn_df[\"churn\"].values X = churn_df[[\"account_length\", \"customer_service_calls\"]].values print(X.shape, y.shape)\n",
        "\n",
        "print (y) print (X)\n",
        "\n",
        "Create a KNN classifier with 6 neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "Fit the classifier to the data\n",
        "knn.fit(X, y) print(churn_df)\n",
        "\n",
        "######################################################\n",
        "########## k-Nearest Neighbors: Predict\n",
        "######################################################\n",
        "\n",
        "#Instructions\n",
        "#Create y_pred by predicting the target values of the unseen features X_new using the knn model.\n",
        "#Print the predicted labels for the set of predictions.\n",
        "#Predict the labels for the X_new\n",
        "\n",
        "predictions_y_test = classifier.predict(X_test)\n",
        "y_pred = knn.predict(X_new)\n",
        "\n",
        "Print the predictions\n",
        "print(\"Predictions: {}\".format(y_pred))\n",
        "\n",
        "######################################################\n",
        "########## Measuring model performance\n",
        "######################################################\n",
        "########## Train/test split + computing accuracy\n",
        "######################################################\n",
        "\n",
        "Instructions\n",
        "Import train_test_split from sklearn.model_selection.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = churn_df.drop(\"churn\", axis=1).values y = churn_df[\"churn\"].values\n",
        "\n",
        "################## Sobre X acima:\n",
        "# churn_df → é um DataFrame do pandas, contendo várias colunas\n",
        "# (features + alvo). .drop(\"churn\", axis=1) → remove a coluna chamada \"churn\".\n",
        "# Ou seja, remove y .values → transforma o DataFrame resultante em um array\n",
        "# NumPy (para ser usado pelo scikit-learn).\n",
        "\n",
        "print (f\"Esses são os valores de X e y\")\n",
        "print(X.shape, y.shape)\n",
        "print (type (X)) print (type (y))\n",
        "\n",
        "# Split X and y into training and test sets, setting test_size equal to 20%, random_state to 42,\n",
        "# and ensuring the target label proportions reflect that of the original dataset.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Ao definir random_state=42 (ou qualquer número inteiro), você garante que a divisão seja reprodutível\n",
        "# ou seja, sempre que rodar o código, os mesmos exemplos irão para treino e os mesmos para teste\n",
        "# PS: O número 42 faz referencia a uma piada apenas, pode ser 21, 88,65,24.....\n",
        "\n",
        "# Sobre o stratify=y\n",
        "# Esse parâmetro garante que a proporção das classes no conjunto de treino e teste seja a mesma do dataset original.\n",
        "# Por exemplo, se o dataset original for 70% da classe \"não churn\" e 30% da classe \"churn\"\n",
        "# O conjunto de treino quanto o de teste terão aproximadamente essa mesma proporção.\n",
        "\n",
        "# Fit the knn model to the training data.\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print the model's accuracy for the test data.\n",
        "\n",
        "print(\"The accurancy is:\")\n",
        "print(knn.score(X_test, y_test))\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Measuring model performance\n",
        "######################################################\n",
        "########## Train/test split + computing accuracy\n",
        "######################################################\n",
        "########## Resolução abaixo é da Datacamp\n",
        "######################################################\n",
        "\n",
        "# Import train_test_split from sklearn.model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import KNeighborsClassifier from sklearn.neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "Separando X e y\n",
        "X = churn_df.drop(\"churn\", axis=1).values y = churn_df[\"churn\"].values\n",
        "\n",
        "################## Sobre X acima:\n",
        "\n",
        "# churn_df → é um DataFrame do pandas, contendo várias colunas (features + alvo).\n",
        "# .drop(\"churn\", axis=1) → remove a coluna chamada \"churn\".\n",
        "# Ou seja, remove y.\n",
        "# .values → transforma o DataFrame resultante em um array NumPy (para ser usado pelo scikit-learn).\n",
        "# print(\"Esses são os valores de X e y\") print(X.shape, y.shape)\n",
        "\n",
        "# Split X and y into training and test sets\n",
        "# test_size = 20% dos dados\n",
        "# random_state = 42 para reprodutibilidade\n",
        "# stratify=y para manter a proporção das classes\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42, stratify=y )\n",
        "\n",
        "# Criando o modelo KNN com K=5\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the knn model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Check the shapes of the training and test sets\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape) print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Check the accuracy output\n",
        "accuracy = knn.score(X_test, y_test) print(\"Accuracy on the test set:\", accuracy)\n",
        "\n",
        "######################################################\n",
        "########## Measuring model performance\n",
        "######################################################\n",
        "########## Overfitting and underfitting\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Instantiate a KNeighborsClassifier, with the number of neighbors equal to the neighbor iterator.\n",
        "# Fit the model to the training data.\n",
        "\n",
        "# Calculate accuracy scores for the training set and test set separately using the .score() method,\n",
        "# and assign the results to the train_accuracies and test_accuracies dictionaries, respectively,\n",
        "# utilizing the neighbor iterator as the index.\n",
        "\n",
        "\n",
        "# Import train_test_split from sklearn.model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Create neighbors as a numpy array of values from 1 up to and including 12.\n",
        "neighbors = np.arange(1, 12)\n",
        "print (\"O arrange criado é: \", neighbors)\n",
        "train_accuracies = {}\n",
        "test_accuracies = {}\n",
        "\n",
        "for neighbor in neighbors:\n",
        "\n",
        "    # Set up a KNN Classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
        "\n",
        "\t  # Fit the model\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "\t  # Compute accuracy\n",
        "\t  train_accuracies[neighbor] = knn.score(X_train, y_train)\n",
        "    test_accuracies[neighbor] = knn.score(X_test, y_test)\n",
        "    print(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Measuring model performance\n",
        "######################################################\n",
        "########## Overfitting and underfitting\n",
        "######################################################\n",
        "########## Resposta da Datacamp do exercício anteior\n",
        "######################################################\n",
        "\n",
        "\n",
        "\n",
        "# Create neighbors\n",
        "neighbors = np.arange(1, 13)\n",
        "train_accuracies = {}\n",
        "test_accuracies = {}\n",
        "\n",
        "for neighbor in neighbors:\n",
        "    # Set up a KNN Classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
        "\n",
        "    # Fit the model\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Compute accuracy\n",
        "    train_accuracies[neighbor] = knn.score(X_train, y_train)\n",
        "    test_accuracies[neighbor] = knn.score(X_test, y_test)\n",
        "\n",
        "# Print the results after the loop\n",
        "print(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n",
        "\n",
        "######################################################\n",
        "########## Measuring model performance\n",
        "######################################################\n",
        "########## Visualizing model complexity\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "\n",
        "\n",
        "# Plot the .values() method of test_accuracies on the y-axis against neighbors on the x-axis, with a label of \"Testing Accuracy\".\n",
        "# Display the plot\n",
        "\n",
        "\n",
        "\n",
        "# Add a title\n",
        "# Add a title \"KNN: Varying Number of Neighbors\"\n",
        "plt.title(\"KNN: Varying Number of Neighbors\")\n",
        "\n",
        "# Plot training accuracies\n",
        "# Plot the .values() method of train_accuracies on the y-axis against neighbors on the x-axis, with a label of \"Training Accuracy\".\n",
        "\n",
        "# print (train_accuracies.values())\n",
        "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
        "# Plot test accuracies\n",
        "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of Neighbors\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UyU9T6HKzQHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to regression**\n",
        "\n",
        "Creating features\n",
        "\n",
        "In this chapter, you will work with a dataset called sales_df, which contains information on advertising campaign expenditure across different media types, and the number of dollars generated in sales for the respective campaign. The dataset has been preloaded for you. Here are the first two rows:\n",
        "\n",
        "     tv        radio      social_media    sales\n",
        "1    13000.0   9237.76    2409.57         46677.90\n",
        "2    41000.0   15886.45   2913.41         150177.83\n",
        "\n",
        "\n",
        "You will use the advertising expenditure as features to predict sales values, initially working with the \"radio\" column. However, before you make any predictions you will need to create the feature and target arrays, reshaping them to the correct format for scikit-learn.\n",
        "\n",
        "\n",
        "# **Building a linear regression model**\n",
        "\n",
        "\n",
        "Now you have created your feature and target arrays, you will train a linear regression model on all feature and target values.\n",
        "\n",
        "As the goal is to assess the relationship between the feature and target values there is no need to split the data into training and test sets.\n",
        "\n",
        "X and y have been preloaded for you as follows:\n",
        "\n",
        "y = sales_df[\"sales\"].values\n",
        "X = sales_df[\"radio\"].values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# **Visualizing a linear regression model**\n",
        "\n",
        "Now you have built your linear regression model and trained it using all available observations, you can visualize how well the model fits the data. This allows you to interpret the relationship between radio advertising expenditure and sales values.\n",
        "\n",
        "The variables X, an array of radio values, y, an array of sales values, and predictions, an array of the model's predicted values for y given X, have all been preloaded for you from the previous exercise.\n",
        "\n",
        "\n",
        "# **Fit and predict for regression**\n",
        "\n",
        "Now you have seen how linear regression works, your task is to create a multiple linear regression model using all of the features in the sales_df dataset, which has been preloaded for you. As a reminder, here are the first two rows:\n",
        "\n",
        "     tv        radio      social_media    sales\n",
        "1    13000.0   9237.76    2409.57         46677.90\n",
        "2    41000.0   15886.45   2913.41         150177.83\n",
        "You will then use this model to predict sales based on the values of the test features.\n",
        "\n",
        "LinearRegression and train_test_split have been preloaded for you from their respective modules.\n",
        "\n",
        "\n",
        "#**Regression Performance**\n",
        "\n",
        "Now you have fit a model, reg, using all features from sales_df, and made predictions of sales values, you can evaluate performance using some common regression metrics.\n",
        "\n",
        "The variables X_train, X_test, y_train, y_test, and y_pred, along with the fitted model, reg, all from the last exercise, have been preloaded for you.\n",
        "\n",
        "Your task is to find out how well the features can explain the variance in the target values, along with assessing the model's ability to make predictions on unseen data.\n",
        "\n",
        "# Significance of MSE\n",
        "MSE serves as a crucial performance metric in regression tasks for several reasons:\n",
        "\n",
        " - Quantifies Prediction Accuracy: MSE provides a measure of how well the model’s predictions align with the actual data points. Lower MSE indicates better predictive accuracy.\n",
        "- Differentiable: Being a differentiable function, MSE facilitates optimization algorithms such as gradient descent for model training.\n",
        "- Loss Function: MSE is commonly used as a loss function in regression algorithms, guiding the learning process towards minimizing prediction errors.\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=fSytzGwwBVw&t=240s\n",
        "\n",
        "Texto: https://www.datageeks.com.br/cross-validation/\n",
        "\n",
        "#**Regularized Regression**\n",
        "\n",
        "Regularized regression (Ridge): https://www.youtube.com/watch?v=Q81RR3yKn30\n",
        "\n",
        "Regularized regression (Lasso):https://www.youtube.com/watch?v=NGf0voTMlcs\n",
        "\n",
        "https://www.appliedaicourse.com/blog/ridge-regression-in-machine-learning/\n",
        "\n",
        "https://www.mygreatlearning.com/blog/what-is-ridge-regression/\n",
        "\n",
        "Ridge Regression vs Lasso Regression: https://www.geeksforgeeks.org/machine-learning/ridge-regression-vs-lasso-regression/\n",
        "\n",
        "\n",
        "#**Curva ROC**\n",
        "\n",
        "https://www.publichealth.columbia.edu/research/population-health-methods/evaluating-risk-prediction-roc-curves\n",
        "\n",
        "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=pt-br\n",
        "\n",
        "https://medium.com/@lg0702/curva-roc-e-au-2ecb57267149\n",
        "\n",
        "https://www.blog.psicometriaonline.com.br/tutorial-curva-roc-diagnostico-clinico-sensibilidade-e-especificidade/\n",
        "\n",
        "https://www.ibm.com/docs/pt-br/spss-statistics/31.0.0?topic=overtraining-roc-curve\n",
        "\n",
        "https://help.sap.com/docs/SAP_PREDICTIVE_ANALYTICS/41d1a6d4e7574e32b815f1cc87c00f42/5e683010bf084133bdbe7c3e80a2c967.html?locale=pt-BR\n",
        "\n",
        "https://learn.microsoft.com/pt-br/shows/machine-learning-for-beginners/analyzing-logistic-regression-performance-with-roc-curves-machine-learning-for-beginners\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_1yRZltqqvoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Creating features\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Create y, an array of the values from the sales_df DataFrame's \"sales\" column.\n",
        "# Reshape X into a two-dimensional NumPy array.\n",
        "# Print the shape of X and y\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create X from the radio column's values\n",
        "# Create X, an array of the values from the sales_df DataFrame's \"radio\" column.\n",
        "\n",
        "X = sales_df[\"radio\"].values\n",
        "print(\"Qual o formato do X?\", X.shape)\n",
        "\n",
        "# Create y from the sales column's values\n",
        "y = sales_df[\"sales\"].values\n",
        "print(\"Qual o formato do y?\", y.shape)\n",
        "\n",
        "\n",
        "# Reshape X\n",
        "X = X.reshape(-1, 1)\n",
        "# Create y from the sales column's values\n",
        "y = sales_df[\"sales\"].values\n",
        "\n",
        "# Reshape X\n",
        "X = X.reshape(-1, 1)\n",
        "\n",
        "# Check the shape of the features and targets\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Building a linear regression model\n",
        "######################################################\n",
        "\n",
        "y = sales_df[\"sales\"].values\n",
        "X = sales_df[\"radio\"].values\n",
        "print(X.shape, y.shape)\n",
        "X = X.reshape(-1, 1)\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# Instructions\n",
        "# Import LinearRegression.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate a linear regression model.\n",
        "# Predict sales values using X, storing as predictions\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X, y)\n",
        "predictions = reg.predict(X)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, predictions)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Visualizing a linear regression model\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Import matplotlib.pyplot as plt.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Loading X and y\n",
        "# Also, making the adjustment\n",
        "y = sales_df[\"sales\"].values\n",
        "X = sales_df[\"radio\"].values\n",
        "X = X.reshape(-1, 1)\n",
        "\n",
        "# Create a scatter plot visualizing y against X, with observations in blue.\n",
        "plt.scatter(X, y, color=\"Blue\")\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X, y)\n",
        "predictions = reg.predict(X)\n",
        "\n",
        "# Draw a red line plot displaying the predictions against X.\n",
        "# Create line plot\n",
        "plt.plot(X, predictions, color=\"red\")\n",
        "plt.xlabel(\"Radio Expenditure ($)\")\n",
        "plt.ylabel(\"Sales ($)\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Fit and predict for regression\n",
        "######################################################\n",
        "\n",
        "\n",
        "# Instantiate a linear regression model.\n",
        "# Fit the model to the training data.\n",
        "# Create y_pred, making predictions for sales using the test features.\n",
        "\n",
        "# importing libraries\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create X, an array containing values of all features in sales_df, and y, containing all values from the \"sales\" column.\n",
        "X = sales_df.drop(\"sales\", axis=1).values\n",
        "y = sales_df[\"sales\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate the model\n",
        "reg = LinearRegression()\n",
        "# Fit the model to the data\n",
        "reg.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = reg.predict(X)\n",
        "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Regression performance\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "\n",
        "# importing libraries\n",
        "# Import root_mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import root_mean_squared_error # https://www.geeksforgeeks.org/machine-learning/calculating-rmse-using-scikit-learn/\n",
        "from sklearn.metrics import mean_squared_error # https://www.geeksforgeeks.org/python/python-mean-squared-error/\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate the model's R-squared score by passing the test feature values and the test target values to an appropriate method.\n",
        "# Compute R-squared\n",
        "\n",
        "# Instantiate the model\n",
        "reg = score\n",
        "# Fit the model to the data\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "r_squared = reg.score(X_test, y_test)\n",
        "# .score => In Python, .score() typically refers to a method used in machine learning libraries,\n",
        "# particularly scikit-learn, to evaluate the performance of a trained model.\n",
        "# The specific metric returned by .score() depends on the type of model:\n",
        "# Classification models: Usually return the mean accuracy (proportion of correctly classified samples).\n",
        "# Regression models: Typically return the coefficient of determination (R² score), which indicates how well the model's predictions approximate the real data points.\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Print the metrics\n",
        "print(\"R^2: {}\".format(r_squared))\n",
        "print(\"RMSE: {}\".format(rmse))\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Cross-validation\n",
        "######################################################\n",
        "########## Cross-validation for R-squared\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "\n",
        "# Import the necessary modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import root_mean_squared_error # https://www.geeksforgeeks.org/machine-learning/calculating-rmse-using-scikit-learn/\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a KFold object\n",
        "# Create kf by calling KFold(), setting the number of splits to six, shuffle to True, and setting a seed of 5.\n",
        "\n",
        "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
        "\n",
        "# Instantiate the model\n",
        "reg = LinearRegression()\n",
        "# Fit the model to the data\n",
        "# reg.fit(X, y) # The cross_val_score function handles fitting internally, so\n",
        "# this step is not necessary.\n",
        "\n",
        "# Perform cross-validation using reg on X and y, passing kf to cv.\n",
        "\n",
        "cv_results = cross_val_score(reg, X, y, cv=kf)\n",
        "\n",
        "# Print scores\n",
        "print(cv_results)\n",
        "print(\"Average 6-Fold CV Score: {}\".format(np.mean(cv_results)))\n",
        "\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Cross-validation\n",
        "######################################################\n",
        "########## Analyzing cross-validation metrics\n",
        "######################################################\n",
        "\n",
        "# IMPORTING LIBRARIES\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Instructions\n",
        "# Calculate and print the mean of the results.\n",
        "\n",
        "# Print the mean\n",
        "print(np.mean(cv_results))\n",
        "\n",
        "# Print the standard deviation\n",
        "print(np.std(cv_results))\n",
        "\n",
        "# Print the 95% confidence interval\n",
        "print(np.quantile(cv_results, [0.025, 0.975]))\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Regularized regression: Ridge\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Import Ridge.\n",
        "\n",
        "# Fit the model to the training data.\n",
        "# Calculate the R^2 score for each iteration of ridge\n",
        "\n",
        "\n",
        "# Import Ridge and others libraries\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Instantiate Ridge, setting alpha equal to alpha.\n",
        "alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
        "ridge_scores = []\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "for alpha in alphas:\n",
        "\n",
        "  # Create a Ridge regression model\n",
        "  ridge = Ridge(alpha=alpha)\n",
        "  ridge.fit(X_train, y_train)\n",
        "  y_pred = ridge.predict(X_test)\n",
        "  ridge_scores.append(ridge.score(X_test, y_test))\n",
        "  print(\"Alpha: {}, Score: {}\".format(alpha, ridge.score(X_test, y_test)))\n",
        "  print(ridge_scores)\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Lasso regression for feature importance\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "\n",
        "# Import Lasso from sklearn.linear_model.\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Instantiate a Lasso regressor with an alpha of 0.3.\n",
        "\n",
        "lasso = Lasso(alpha=0.3)\n",
        "lasso = lasso.fit(X, y)\n",
        "lasso_coef = lasso.fit(X, y).coef_ # Coeficientes do Lasso\n",
        "\n",
        "print(lasso_coef)\n",
        "plt.bar(sales_columns, lasso_coef)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Em termos práticos:\n",
        "# Use .fit(X, y).coef_ se você quer direto os coeficientes.\n",
        "# Use .fit(X, y) se você quer guardar o modelo treinado\n",
        "\n",
        "scores_lasso = []\n",
        "for alpha in [0.3]:\n",
        "  lasso = Lasso (alpha=alpha)\n",
        "  lasso.fit(X_train, y_train)\n",
        "  lasso_predict=Lasso.predict(X_test)\n",
        "  scores_lasso.append(lasso.score(X_test,y_test))\n",
        "  print(\"Alpha:{}, Score:{}\".format(alpha, scores_lasso)\n",
        "  print(scores_lasso)\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## How good is your model?\n",
        "######################################################\n",
        "# Site interessante: https://www.geeksforgeeks.org/machine-learning/confusion-matrix-machine-learning/\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "\n",
        "# Predict the labels of the test set, storing the results as y_pred.\n",
        "# Compute and print the confusion matrix and classification report for the test labels versus the predicted labels.\n",
        "\n",
        "# Import confusion_matrix and classification_report.\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test data: y_pred\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix and classification report\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Logistic regression and the ROC curve\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Import LogisticRegression.\n",
        "# Instantiate a logistic regression model, logreg.\n",
        "# Fit the model to the training data.\n",
        "# Predict the probabilities of each individual in the test set having a diabetes diagnosis,\n",
        "# storing the array of positive probabilities as y_pred_probs.\n",
        "\n",
        "\n",
        "# Import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate the model\n",
        "logreg = LogisticRegression()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probabilities of each individual in the test set having a diabetes\n",
        "# diagnosis, storing the array of positive probabilities as y_pred_probs.\n",
        "# Predict probabilities\n",
        "\n",
        "y_pred = logreg.predict(X_test) # Valores \"0\" e \"1\"\n",
        "y_pred_probs = logreg.predict_proba(X_test)[:, 1]\n",
        "# predict_proba is a machine learning function (common in scikit-learn) that\n",
        "# returns the probability of each input sample belonging to each class,\n",
        "\n",
        "# [:, 1]  (:) Means all row\n",
        "#         (1) Means the only the second column - INdex 0\n",
        "\n",
        "print(y_pred_probs[:10])\n",
        "# [:10]: Select the first 10 elements (or rows/samples) of the array/tensor.\n",
        "\n",
        "# y_pred = logreg.predict(X_test)\n",
        "# Essa linha classifica diretamente cada observação do conjunto de teste como 0 ou 1.\n",
        "# O modelo usa internamente as probabilidades calculadas e aplica um limiar padrão de 0.5:\n",
        "# Se a probabilidade da classe positiva ≥ 0.5 → retorna 1.\n",
        "# Caso contrário → retorna 0.\n",
        "# Resultado: um vetor binário, por exemplo [0, 0, 1, 0, 1, ...].\n",
        "\n",
        "\n",
        "# O predict_proba gera uma matriz com duas colunas:\n",
        "# Resultado: um vetor contínuo de valores entre 0 e 1, por exemplo [0.26, 0.18, 0.12, 0.79, ...].\n",
        "# Isso é essencial quando você quer:\n",
        "# Avaliar confiança da previsão.\n",
        "# Construir métricas como curva ROC, AUC, precisão-recall.\n",
        "# Ajustar o limiar de decisão (não precisa ser 0.5; pode ser 0.3 ou 0.7 dependendo do problema).\n",
        "# Ordenar indivíduos por risco (ex.: quem tem maior probabilidade de diabetes).\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## The ROC curve\n",
        "######################################################\n",
        "\n",
        "# Import roc_curve.\n",
        "# Calculate the ROC curve values, using y_test and y_pred_probs, and unpacking the results into fpr, tpr, and thresholds.\n",
        "# Plot true positive rate against false positive rate.\n",
        "\n",
        "\n",
        "# Import roc_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Generate ROC curve values: fpr, tpr, thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "# Plot tpr against fpr\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Diabetes Prediction')\n",
        "plt.show()\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## ROC AUC\n",
        "##############################################\n",
        "\n",
        " # https://www.leansaude.com.br/o-que-e-curva-roc-sensibilidade-e-especificidade/\n",
        "\n",
        "# Import roc_auc_score.\n",
        "# Calculate and print the ROC AUC score, passing the test labels and the predicted positive class probabilities.\n",
        "# Calculate and print the confusion matrix.\n",
        "# Call classification_report().\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(roc_auc_score(y_test, y_pred_probs))\n",
        "\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the classification report\n",
        "print(____(____, ____))\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Hyperparameter tuning\n",
        "######################################################\n",
        "\n",
        "########## Materials\n",
        "########## https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74/\n",
        "########## https://medium.com/@aditib259/a-comprehensive-guide-to-hyperparameter-tuning-in-machine-learning-dd9bb8072d02\n",
        "########## https://www.geeksforgeeks.org/machine-learning/hyperparameter-tuning/\n",
        "########## https://aws.amazon.com/what-is/hyperparameter-tuning/\n",
        "\n",
        "######################################################\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Hyperparameter tuning with GridSearchCV\n",
        "######################################################\n",
        "\n",
        "# Now you have seen how to perform grid search hyperparameter tuning, you are going to build a lasso regression model\n",
        "# with optimal hyperparameters to predict blood glucose levels using the features in the diabetes_df dataset.\n",
        "# X_train, X_test, y_train, and y_test have been preloaded for you.\n",
        "# A KFold() object has been created and stored for you as kf, along with a lasso regression model as lasso.\n",
        "\n",
        "# Instructions\n",
        "\n",
        "# A) Import GridSearchCV.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# B) Set up a parameter grid for \"alpha\", using np.linspace() to create 20 evenly spaced values ranging from 0.00001 to 1.\n",
        "param_grid = {\"alpha\": np.linspace(0.00001,1,20)}\n",
        "lasso = Lasso()\n",
        "\n",
        "# C) Call GridSearchCV(), passing lasso, the parameter grid, and setting cv equal to kf.\n",
        "lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n",
        "\n",
        "# D) Fit the grid search object to the training data to perform a cross-validated grid search.\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "# E) Fit to the training data\n",
        "\n",
        "print(\"Tuned lasso paramaters - alpha: {}\".format(lasso_cv.best_params_))\n",
        "print(\"Tuned lasso score R^2: {}\".format(lasso_cv.best_score_))\n",
        "\n",
        "\n",
        "# Exploring the GridSearchCV Class\n",
        "# GridSearchCV(\n",
        "#    estimator=,     # A sklearn model\n",
        "#    param_grid=,    # A dictionary of parameter names and values\n",
        "#    cv=,            # An integer that represents the number of k-folds\n",
        "#    scoring=,       # The performance measure (such as r2, precision)\n",
        "#    n_jobs=,        # The number of jobs to run in parallel\n",
        "#    verbose=        # Verbosity (0-3, with higher being more)\n",
        "# )\n",
        "\n",
        "######################################################\n",
        "########## Introduction to regression\n",
        "######################################################\n",
        "########## Hyperparameter tuning with RandomizedSearchCV\n",
        "######################################################\n",
        "\n",
        "# Training and test sets from diabetes_df have been pre-loaded for you as X_train. X_test, y_train, and y_test,\n",
        "# where the target is \"diabetes\". A logistic regression model has been created\n",
        "# and stored as logreg, as well as a KFold variable stored as kf.\n",
        "\n",
        "# You will define a range of hyperparameters and use RandomizedSearchCV,\n",
        "# which has been imported from sklearn.model_selection, to look for optimal\n",
        "# hyperparameters from these options.\n",
        "\n",
        "\n",
        "# Instructions\n",
        "# A) Create params, adding \"l1\" and \"l2\" as penalty values, setting C to a range of 50 float\n",
        "# values between 0.1 and 1.0, and class_weight to either \"balanced\" or a dictionary containing 0:0.8, 1:0.2.\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params = {\"penalty\": [\"l1\", \"l2\"],\n",
        "         \"tol\": np.linspace(0.0001, 1.0, 50),\n",
        "         \"C\": np.linspace(0.1, 1.0, 50),\n",
        "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n",
        "\n",
        "# Instantiate the RandomizedSearchCV object\n",
        "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
        "\n",
        "\n",
        "# Fit the data to the model\n",
        "logreg_cv.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Print the tuned parameters and score\n",
        "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
        "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Preprocessing data\n",
        "######################################################\n",
        "########## Creating dummy variables\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Use a relevant function, passing the entire music_df DataFrame, to create music_dummies, dropping the first binary column.\n",
        "# Print the shape of music_dummies.\n",
        "\n",
        "\n",
        "# Create music_dummies\n",
        "# import pandas as pd\n",
        "# music_df = pd.read_csv('music.csv')\n",
        "\n",
        "music_dummies = pd.get_dummies(music_df, drop_first=True)\n",
        "\n",
        "# Print the new DataFrame's shape\n",
        "print(\"Shape of music_dummies: {}\".format(music_dummies.shape))\n",
        "print(\"Shape of music_df: {}\".format(music_df.shape))\n",
        "\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Regression with categorical features\n",
        "######################################################\n",
        "\n",
        "# music_dummies has been preloaded for you, along with Ridge, cross_val_score, numpy as np, and a KFold object stored as kf.\n",
        "\n",
        "# The model will be evaluated by calculating the average RMSE, but first, you will need to convert the scores for each fold\n",
        "# to positive values and take their square root. This metric shows the average error of our model's predictions,\n",
        "# so it can be compared against the standard deviation of the target value—\"popularity\".\n",
        "\n",
        "\n",
        "# from sklearn.model_selection import cross_val_score, KFold\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create X and y\n",
        "\n",
        "X = music_dummies.drop(\"popularity\", axis=1).values\n",
        "print(\"Spreadsheet X: {}\", X)\n",
        "y = music_dummies[\"popularity\"].values\n",
        "print(\"\\n\")\n",
        "print(\"Spreadsheet y: {}\", y)\n",
        "\n",
        "# Preparing the Trainning Data and Test Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
        "\n",
        "# Instantiate a ridge model\n",
        "ridge = Ridge(alpha=0.2)\n",
        "\n",
        "# Train the model\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
        "print(\"MSE scores: {}\".format(scores))\n",
        "\n",
        "# Calculate RMSE\n",
        "print(\"\\n\")\n",
        "rmse = np.sqrt(-scores)\n",
        "print(\"rmse:\",rmse)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Average RMSE: {}\".format(np.mean(rmse)))\n",
        "print(\"Standard Deviation of the target array: {}\".format(np.std(y)))\n",
        "\n",
        "print(\"Valor do score:\",scores)\n",
        "\n",
        "\n",
        "# --- Ação para acessar os coeficientes ---\n",
        "\n",
        "# Coeficientes\n",
        "coeficientes = ridge.coef_\n",
        "\n",
        "# Intercepto (o termo constante)\n",
        "intercepto = ridge.intercept_\n",
        "\n",
        "# Imprimir os resultados (opcional)\n",
        "print(\"\\n\")\n",
        "print(f\"Intercepto (b0): {intercepto}\")\n",
        "print(f\"Coeficientes (beta): {coeficientes}\")\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Handling missing data\n",
        "######################################################\n",
        "########## Dropping missing data\n",
        "######################################################\n",
        "\n",
        "######################################################\n",
        "########## Websites - Explanation\n",
        "######################################################\n",
        "########## https://www.geeksforgeeks.org/machine-learning/handling-missing-values-machine-learning/\n",
        "########## https://medium.com/@pingsubhak/handling-missing-values-in-dataset-7-methods-that-you-need-to-know-5067d4e32b62\n",
        "########## https://link.springer.com/article/10.1186/s40537-021-00516-9\n",
        "########## https://towardsdatascience.com/how-to-handle-missing-data-b557c9e82fa0/\n",
        "########## https://sbic.org.br/lnlm/wp-content/uploads/2022/12/vol20-no1-art3.pdf\n",
        "#####################################################\n",
        "\n",
        "# Instructions 1/3\n",
        "# Print the number of missing values for each column in the music_df dataset, sorted in ascending order.\n",
        "# music_df dataset\n",
        "\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "# Instructions 2/3\n",
        "# Remove values for all columns with 50 or fewer missing values.\n",
        "\n",
        "# Print missing values for each column\n",
        "print(music_df.isna().sum().sort_values())\n",
        "# isna() is a widely used function within the pandas library for detecting missing or null values\n",
        "music_df_orig = music_df\n",
        "\n",
        "# Remove values where less than 5% are missing\n",
        "# This 5% needs to be calculated on Excel sheet\n",
        "music_df_alt = music_df.dropna(subset=[\"genre\",\"popularity\",\"loudness\",\"liveness\",\"tempo\"])\n",
        "print(music_df_alt.isna().sum().sort_values())\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "\n",
        "# Instructions\n",
        "# Convert music_df[\"genre\"] to values of 1 if the row contains \"Rock\", otherwise change the value to 0.\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Print missing values for each column\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "# Remove values where less than 5% are missing\n",
        "music_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n",
        "\n",
        "# Convert genre to a binary feature\n",
        "music_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n",
        "\n",
        "# music_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n",
        "\n",
        "# X = music_df.drop(\"Rock\", axis=1).values\n",
        "# print(\"Spreadsheet X: {}\", X)\n",
        "\n",
        "print(music_df.isna().sum().sort_values())\n",
        "print(\"Shape of the `music_df`: {}\".format(music_df.shape))\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Handling missing data\n",
        "######################################################\n",
        "########## Pipeline for song genre prediction: I\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "\n",
        "\n",
        "# Import modules\n",
        "# Import SimpleImputer and Pipeline.\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# SimpleImputer => Handling missing values in a dataset by replacing them with a simple descriptive\n",
        "# statistic (like the mean, median, or most frequent value) along each column\n",
        "\n",
        "# Instantiate an imputer\n",
        "imputer = SimplerImputer()\n",
        "\n",
        "# Instantiate a knn model\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Fit the knn model to the training data.\n",
        "\n",
        "# Build steps for the pipeline\n",
        "steps = [(\"imputer\", imputer),\n",
        "         (\"knn\", knn)]\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Handling missing data\n",
        "######################################################\n",
        "########## Pipeline for song genre prediction: II\n",
        "######################################################\n",
        "\n",
        "# Instructions\n",
        "# Create a pipeline using the steps you previously defined.\n",
        "# Fit the pipeline to the training data.\n",
        "# Make predictions on the test set.\n",
        "# Calculate and print the confusion matrix.\n",
        "\n",
        "# X_train, X_test, y_train, and y_test have been preloaded for you\n",
        "\n",
        "steps = [(\"imputer\", imp_mean),\n",
        "        (\"knn\", knn)]\n",
        "\n",
        "        print(imp_mean)\n",
        "\n",
        "# https://www.geeksforgeeks.org/machine-learning/what-is-exactly-sklearnpipelinepipeline/\n",
        "# https://medium.com/data-hackers/como-usar-pipelines-no-scikit-learn-1398a4cc6ae9\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Centering and scaling\n",
        "######################################################\n",
        "########## Centering and scaling for regression\n",
        "######################################################\n",
        "########## https://towardsdatascience.com/data-preprocessing-with-scikit-learn-standardization-and-scaling-cfb695280412/\n",
        "########## https://www.geeksforgeeks.org/machine-learning/data-pre-processing-wit-sklearn-using-standard-and-minmax-scaler/\n",
        "########## https://towardsdatascience.com/data-leakage-in-machine-learning-6161c167e8ba/\n",
        "######################################################\n",
        "\n",
        "# X_train, X_test, y_train, and y_test have been created from the music_df dataset,\n",
        "# where the target is \"loudness\" and the features are all other columns in the dataset.\n",
        "# Lasso and Pipeline have also been imported for you.\n",
        "\n",
        "# Note that \"genre\" has been converted to a binary feature where 1 indicates a rock song, and 0 represents other genres.\n",
        "\n",
        "# Instructions\n",
        "\n",
        "\n",
        "# C) Instantiate a pipeline with steps to scale and build a lasso regression model.\n",
        "# D) Calculate the R-squared value on the test data.\n",
        "\n",
        "\n",
        "# Import StandardScaler\n",
        "# A) Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the steps for the pipeline object, a StandardScaler object called \"scaler\",\n",
        "# and a lasso model called \"lasso\" with alpha set to 0.5.\n",
        "\n",
        "steps = [(\"scaler\", StandardScaler()),\n",
        "         (\"lasso\", Lasso(alpha=0.5))]\n",
        "\n",
        "# Instantiate the pipeline\n",
        "pipeline = Pipeline (steps)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print R-squared\n",
        "print(pipeline.score(X_test, y_test))\n",
        "\n",
        "\n",
        "######################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Centering and scaling for classification\n",
        "######################################################\n",
        "\n",
        "# Build the steps\n",
        "# Build the steps for the pipeline: a StandardScaler() object named \"scaler\",\n",
        "# and a logistic regression model named \"logreg\".\n",
        "\n",
        "steps = [(\"scaler\", StandardScaler()),\n",
        "         (\"logreg\", LogisticRegression())]\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Create the parameters, searching 20 equally spaced float values ranging from\n",
        "# 0.001 to 1.0 for the logistic regression model's C hyperparameter within the pipeline.\n",
        "\n",
        "parameters = {\"tol\": np.linspace(0.0001, 1.0, 20))}\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=21)\n",
        "\n",
        "# Instantiate the grid search object\n",
        "cv = GridSearchCV(pipeline, parameters)\n",
        "\n",
        "# Fit to the training data\n",
        "cv.fit (X_train, y_train)\n",
        "print(cv.best_score_, \"\\n\", cv.best_params_)\n",
        "\n",
        "\n",
        "#####################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Visualizing regression model performance\n",
        "######################################################\n",
        "\n",
        "# The music_df dataset has had dummy variables for \"genre\" added\n",
        "# Also, feature and target arrays have been created => Split into X_train, X_test, y_train, and y_test.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Write a for loop using model as the iterator, and model.values() as the iterable.\n",
        "models = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(alpha=0.1), \"Lasso\": Lasso(alpha=0.1)}\n",
        "results = []\n",
        "\n",
        "# Loop through the models' values\n",
        "for model in models.values():\n",
        "  kf = KFold(n_splits=6, random_state=42, shuffle=True)\n",
        "  cv_results = cross_val_score(model, X_train, y_train, cv=kf)\n",
        "  results.append(cv_results)\n",
        "print(\"\\n\")\n",
        "print(\"Valores dos Resultados:\",results)\n",
        "\n",
        "# Create a box plot of the results\n",
        "plt.boxplot(results, labels=models.keys())\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## VPredicting on the test set\n",
        "######################################################\n",
        "\n",
        "# Import root_mean_squared_error\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "for name, model in models.items():\n",
        "# Fit the model to the scaled training features and the training labels.\n",
        "\n",
        "  # Fit the model to the training data\n",
        "  model.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = model.predict(X_test_scaled) # Valores gerados com o X_test\n",
        "\n",
        "  # Calculate the test_rmse\n",
        "  test_rmse = root_mean_squared_error(y_test, y_pred)\n",
        "  # Avaliação de Desempenho: O MSE fornece uma medida quantitativa da # precisão do modelo.\n",
        "  # Quanto menor o valor do MSE, mais próximas estão as previsões do  # modelo dos dados reais, indicando um melhor ajuste.\n",
        "\n",
        "  print(\"{} Test Set RMSE: {}\".format(name, test_rmse))\n",
        "\n",
        "#####################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Visualizing classification model performance\n",
        "######################################################\n",
        "\n",
        "# Create a dictionary of \"Logistic Regression\", \"KNN\", and \"Decision Tree Classifier\",\n",
        "# setting the dictionary's values to a call of each model.\n",
        "# Create models dictionary\n",
        "\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\": DecisionTreeClassifier()}\n",
        "results = []\n",
        "\n",
        "# Loop through the models' values\n",
        "for name, model in models.items():\n",
        "\n",
        "  kf = KFold(n_splits=6, random_state=12, shuffle=True)\n",
        "\n",
        "  # Perform cross-validation\n",
        "  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
        "  print (\"Valor do CV_Results para {} : {}\".format(name,cv_results))\n",
        "  print(\"\\n\")\n",
        "  results.append(cv_results)\n",
        "plt.boxplot(results, labels=models.keys())\n",
        "plt.title('Comparação de Desempenho: Modelos de Classificação', fontsize=14)\n",
        "plt.ylabel('Acurácia por Fold (6 folds)', fontsize=12)\n",
        "plt.xlabel('Modelo', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Linhas de grade horizontais\n",
        "plt.ylim(0.6, 0.85)  # Ajuste os limites para focar na faixa relevante\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "########## Supervised Learning with scikit-learn\n",
        "######################################################\n",
        "########## Pipeline for predicting song popularity\n",
        "######################################################\n",
        "\n",
        "\n",
        "######################################################\n",
        "#################### Instructions ####################\n",
        "######################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create steps\n",
        "# Create the steps for the pipeline by calling a simple imputer, a standard scaler, and a logistic regression model.\n",
        "steps = [(\"imp_mean\", SimpleImputer()),\n",
        "         (\"scaler\", StandardScaler()),\n",
        "         (\"logreg\", LogisticRegression())]\n",
        "\n",
        "# Set up pipeline\n",
        "# Create a pipeline object, and pass the steps variable.\n",
        "pipeline = Pipeline (steps)\n",
        "params = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n",
        "         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "# https://medium.com/@Mandeep2002/gridsearchcv-key-concepts-8f98ceb633e4\n",
        "# Instantiate a grid search object to perform cross-validation using the pipeline and the parameters.\n",
        "tuning = GridSearchCV(pipeline, param_grid=params)\n",
        "tuning.fit(X_train, y_train)\n",
        "y_pred = tuning.predict(X_test)\n",
        "\n",
        "# Exploring the GridSearchCV Class\n",
        "# GridSearchCV(\n",
        "  #  estimator=,     # A sklearn model\n",
        "  #  param_grid=,    # A dictionary of parameter names and values\n",
        "  #  cv=,            # An integer that represents the number of k-folds\n",
        "  #  scoring=,       # The performance measure (such as r2, precision)\n",
        "  #  n_jobs=,        # The number of jobs to run in parallel\n",
        "  #  verbose=        # Verbosity (0-3, with higher being more)\n",
        "\n",
        "# Compute and print performance\n",
        "# Print the best parameters and compute and print the test set accuracy score for the grid search object.\n",
        "print(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))\n"
      ],
      "metadata": {
        "id": "nGuj2vNXqxb2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}